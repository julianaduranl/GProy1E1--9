{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3a39300",
   "metadata": {},
   "source": [
    "# Proyecto 1\n",
    "El Fondo de Población de las Naciones Unidas (UNFPA), en el marco de la Agenda 2030, busca \n",
    "aprovechar las opiniones ciudadanas para identificar problemáticas relacionadas con los \n",
    "Objetivos de Desarrollo Sostenible (ODS). En particular, se requiere un sistema capaz de analizar \n",
    "y clasificar textos en tres categorías:\n",
    "\n",
    "- **ODS 1**: Fin de la pobrez\n",
    "- **ODS 3**: Salud y bienestar\n",
    "- **ODS 4**: Educación de calidad\n",
    "\n",
    "El reto consiste en desarrollar un modelo de analítica de textos que reciba como entrada una \n",
    "opinión en lenguaje natural y prediga automáticamente a cuál ODS pertenece. Este modelo \n",
    "debe ser lo suficientemente robusto para manejar textos largos, con ruido (referencias, \n",
    "paréntesis, números) y en idioma español."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636baa60",
   "metadata": {},
   "source": [
    "## 0. Reflexión sobre el proceso y conocimientos requeridos\n",
    "\n",
    "Para resolver este caso, se debe seguir un proceso típico de **analítica de textos**:\n",
    "\n",
    "1. **Entendimiento del negocio y los datos**  \n",
    "   Comprender el contexto (ODS) y explorar la distribución de las etiquetas.\n",
    "\n",
    "2. **Preprocesamiento de textos**  \n",
    "   Normalización, limpieza de ruido, tokenización, eliminación de stopwords, lematización \n",
    "   en español y construcción de bigramas relevantes.\n",
    "\n",
    "3. **Modelado**  \n",
    "   Probar al menos tres algoritmos de clasificación supervisada (Naive Bayes, SVM, Random Forest, \n",
    "   redes neuronales, etc.), comparando métricas.\n",
    "\n",
    "4. **Evaluación y resultados**  \n",
    "   Seleccionar el mejor modelo, justificar métricas y explicar qué palabras o frases \n",
    "   llevan a la clasificación en cada ODS.\n",
    "\n",
    "5. **Automatización y despliegue (Etapa 2)**  \n",
    "   Construir un pipeline reproducible, crear una API REST y desplegar una aplicación web/móvil.\n",
    "\n",
    "### Conocimientos requeridos:\n",
    "- Manejo de librerías de NLP en Python (`nltk`, `spacy`, `scikit-learn`).\n",
    "- Técnicas de limpieza de texto en español.\n",
    "- Algoritmos de clasificación supervisada.\n",
    "- Conceptos básicos de APIs y despliegue en aplicaciones.\n",
    "\n",
    "### Dudas actuales:\n",
    "- ¿Qué técnica de vectorización será más efectiva: TF-IDF o embeddings?\n",
    "- ¿Qué tan balanceadas están las clases (ODS 1, 3, 4)?\n",
    "- ¿Cuánta aumentación de datos será necesaria para mejorar la clase minoritaria?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ff7786",
   "metadata": {},
   "source": [
    "### 1. Instalación e importanción de librerías.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31bac332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Instalación (mac) ---\n",
    "# !python3 -m pip install -U nltk spacy scikit-learn matplotlib seaborn unidecode\n",
    "# !python3 -m spacy download es_core_news_md\n",
    "\n",
    "# --- Imports base ---\n",
    "import re, string, unicodedata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "# Si quisieras stemming en español (no recomendado para el pipeline final):\n",
    "# from nltk.stem import SnowballStemmer\n",
    "\n",
    "import spacy\n",
    "# Carga del modelo de spaCy en español (md tiene vectores; lg si quieres mejores)\n",
    "nlp = spacy.load(\"es_core_news_md\")\n",
    "\n",
    "# Utilidad: solo úsalo DESPUÉS de lematizar si quieres una segunda versión sin acentos\n",
    "import unidecode\n",
    "\n",
    "# Sklearn: vectorización y modelos\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# NLTK data \n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "# Stopwords en español \n",
    "STOP_ES = set(stopwords.words('spanish'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (mushroom)",
   "language": "python",
   "name": "mushroom-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
